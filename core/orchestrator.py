from core.functions import function_registry
from core.langchain import run_llm_chain
from core.prompts import INTENT_PROMPT, ANSWER_PROMPT
from langchain.prompts import ChatPromptTemplate
from utils.date import resolve_date
import asyncio
import json


DEFAULT_ERROR_RES = "Desculpe, nÃ£o consegui entender a sua mensagem."


async def orchestrate_user_prompt(user_prompt: str) -> str:
    """
    Orchestrates the user's prompt by determining intent and executing the appropriate functions.

    Args:
        user_prompt (str): the user's message request.

    Returns:
        str: the combined response from the executed functions.
    """
    function_args = await __determine_intent(user_prompt)
    return await __execute_functions(function_args, user_prompt)


async def __determine_intent(user_prompt: str) -> list[str]:
    """
    Determines which functions to call based on the user's prompt using LangChain.

    Args:
        user_prompt (str): the user's message request.

    Returns:
        list[str]: a list of function names determined from the prompt.
    """
    function_descriptions = [
        f"{name}: {info['description']}" for name, info in function_registry.items()
    ]

    prompt_template = ChatPromptTemplate.from_template(INTENT_PROMPT)

    response = await run_llm_chain(
        prompt_template,
        {"user_prompt": user_prompt, "functions": "\n".join(function_descriptions)},
    )

    try:
        return json.loads(response)
    except json.JSONDecodeError:
        return {}


async def __execute_functions(function_args: dict, user_prompt: str) -> str:
    """
    Executes the functions identified by the intent determination, using dynamic arguments,
    and generates a natural-language response using the LLM.

    Args:
        function_args (dict): A dictionary where keys are function names (str) and values are
                              argument dictionaries to pass to the corresponding functions.
        user_prompt (str): The original user query.

    Returns:
        str: The final response generated by the LLM, or a fallback message if no valid functions are found.
    """
    if not function_args or not isinstance(function_args, dict):
        return DEFAULT_ERROR_RES

    tasks = []
    for fn_name, args in function_args.items():
        func_info = function_registry.get(fn_name)

        if func_info:
            func = func_info["func"]

            if not isinstance(args, dict):
                args = {}

            if "date" in args:
                args["date"] = resolve_date(args["date"])

            tasks.append(func(**args))

    if not tasks:
        return DEFAULT_ERROR_RES

    results = await asyncio.gather(*tasks)

    # Prepare the function results as context
    fn_results = {fn_name: res for fn_name, res in zip(function_args.keys(), results)}

    prompt_template = ChatPromptTemplate.from_template(ANSWER_PROMPT)

    response = await run_llm_chain(
        prompt_template,
        {"user_prompt": user_prompt, "fn_results": json.dumps(fn_results, indent=2)},
    )

    return response or DEFAULT_ERROR_RES
